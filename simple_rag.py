#!/usr/bin/env python3
"""
ç®€åŒ–çš„ RAG æŸ¥è¯¢æ–¹å¼
ä½¿ç”¨ Ollama æœ¬åœ° LLM ç›´æ¥å¤„ç†ä¸Šä¸‹æ–‡ï¼Œé¿å…å¤æ‚çš„é“¾é—®é¢˜
"""

from config import Config
from vector_store import VectorStore
from rag_assistant import RAGAssistant
from langchain_community.llms import Ollama
import time


def simple_rag_query(question: str, k: int = 3) -> dict:
    """ç®€åŒ–çš„ RAG æŸ¥è¯¢
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
        
    Returns:
        åŒ…å«ç­”æ¡ˆå’Œæ¥æºçš„å­—å…¸
    """
    print(f"\nğŸ“ é—®é¢˜: {question}\n")
    
    # 0. ä¼˜åŒ–æŸ¥è¯¢
    optimized_q = RAGAssistant.optimize_query(question)
    if optimized_q != question:
        print(f"âœ“ æŸ¥è¯¢ä¼˜åŒ–: '{question}' â†’ '{optimized_q}'")
        search_query = optimized_q
    else:
        search_query = question
    
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    print("ğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
    vector_store = VectorStore()
    docs = vector_store.similarity_search(search_query, k=k)
    
    if not docs:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return {"question": question, "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", "sources": []}
    
    print(f"âœ… æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£\n")
    
    # 2. ç»„ç»‡ä¸Šä¸‹æ–‡
    context_parts = []
    sources = []
    for i, doc in enumerate(docs, 1):
        content = doc.page_content
        source = doc.metadata.get('source', 'æœªçŸ¥') if hasattr(doc, 'metadata') else 'æœªçŸ¥'
        context_parts.append(f"ã€æ–‡æ¡£{i}ã€‘{source}\n{content}")
        sources.append(doc)
    
    context = "\n\n".join(context_parts)
    
    # 3. æ„å»ºæç¤º
    prompt = f"""æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘
{context}

ã€é—®é¢˜ã€‘
{question}

ã€å›ç­”ã€‘
è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ç»™å‡ºå‡†ç¡®çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
    
    # 4. è°ƒç”¨ LLM
    print("ğŸ¤– LLM ç”Ÿæˆç­”æ¡ˆ...")
    start = time.time()
    
    llm = Ollama(
        base_url=Config.OLLAMA_API_URL,
        model=Config.OLLAMA_MODEL,
        temperature=Config.TEMPERATURE,
        num_predict=Config.MAX_TOKENS,
    )
    
    try:
        answer = llm.invoke(prompt)
        elapsed = time.time() - start
        
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼ˆè€—æ—¶ {elapsed:.2f}ç§’ï¼‰\n")
        
        return {
            "question": question,
            "answer": answer,
            "sources": sources,
            "elapsed": elapsed
        }
        
    except Exception as e:
        print(f"âŒ LLM ç”Ÿæˆå¤±è´¥: {e}\n")
        return {
            "question": question,
            "answer": f"LLM ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ Ollama è¿æ¥ã€‚é”™è¯¯: {e}",
            "sources": sources,
            "error": str(e)
        }


def main():
    print("\n" + "="*80)
    print("ç®€åŒ– RAG æŸ¥è¯¢æµ‹è¯•")
    print("="*80)
    
    queries = [
        "æ·±åº¦å­¦ä¹ çš„ä¸»è¦æ¶æ„æœ‰å“ªäº›ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯ CNN å’Œ RNNï¼Ÿ",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
    ]
    
    for query in queries:
        result = simple_rag_query(query, k=3)
        
        print("="*80)
        print(f"\nç­”æ¡ˆ:")
        print("-"*80)
        answer = result.get('answer', 'æ— ç­”æ¡ˆ')
        print(answer)
        
        if 'sources' in result and result['sources']:
            print(f"\nğŸ“š å‚è€ƒæ¥æº ({len(result['sources'])} ä¸ª):")
            print("-"*80)
            for i, doc in enumerate(result['sources'], 1):
                if hasattr(doc, 'metadata'):
                    source = doc.metadata.get('source', 'æœªçŸ¥')
                else:
                    source = 'æœªçŸ¥'
                content = doc.page_content[:100].replace('\n', ' ') if hasattr(doc, 'page_content') else ''
                print(f"  [{i}] {source}")
                print(f"      {content}...\n")
        
        if 'elapsed' in result:
            print(f"â±ï¸  è€—æ—¶: {result['elapsed']:.2f}ç§’\n")
        
        print("\n")


if __name__ == "__main__":
    main()
